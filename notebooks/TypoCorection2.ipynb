{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqOBWOEJ6JHq",
        "outputId": "5ee7585d-00b2-4870-a195-d0bcea715099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting JPype1\n",
            "  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/488.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/488.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m460.8/488.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1) (23.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install JPype1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku4qinox6PNe"
      },
      "outputs": [],
      "source": [
        "import re, os, pickle\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation, digits\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation, digits\n",
        "import jpype as jp\n",
        "import string\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzSCbDni57sC"
      },
      "outputs": [],
      "source": [
        "# Make sure to replace the path with the path to your specific file\n",
        "file_path = '/content/drive/MyDrive/data/Data.csv'\n",
        "df = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr0de5ChICP_"
      },
      "outputs": [],
      "source": [
        "def preprocess(df):\n",
        "\n",
        "  df = df[['text','sentiment']]\n",
        "  texts = df['text']\n",
        "\n",
        "  # drop same texts\n",
        "  df.drop_duplicates(subset = ['text'], inplace = True)\n",
        "\n",
        "  df['sentiment'] = df['sentiment'].str.lower()\n",
        "  df['sentiment'] = df['sentiment'].map({'sexist': 'sexist', 'insult': 'insult', 'racist': 'racist', 'profanity': 'profanity'\n",
        "  , 'not-offensive': 'notoffensive', \"notoffensive\": \"notoffensive\", \"not offensive\" : \"notoffensive\",\"ınsult\": \"insult\"})\n",
        "    # Create dummy columns without dropping any columns\n",
        "  df_dummies = pd.get_dummies(df['sentiment'], drop_first=False)\n",
        "\n",
        "  # Concatenate the dummy columns with the original DataFrame\n",
        "  df = pd.concat([df, df_dummies], axis=1)\n",
        "  df = df.dropna(subset=['sentiment'])\n",
        "  df.drop(columns = ['sentiment'],inplace = True)\n",
        "\n",
        "  # Remove words that contain a '#' in them entirely\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda text: re.sub(r\"#\\S+\", \"\", text))\n",
        "\n",
        "  # Remove rows with less than 5 characters\n",
        "  df = df[df['text'].str.len() > 5]\n",
        "\n",
        "  # Remove rows with only punctuation\n",
        "  df = df[~df['text'].str.contains(r'^[\\W_]+$')]\n",
        "\n",
        "  # Remove rows with only whitespace\n",
        "  df = df[~df['text'].str.isspace()]\n",
        "\n",
        "  # Remove rows with only digits\n",
        "  df = df[~df['text'].str.isdigit()]\n",
        "\n",
        "  # Apply the function to the \"text\" column\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda text: re.sub(r\"@\\S+\", \"\", text))\n",
        "\n",
        "  # Remove punctuation from the 'text' column\n",
        "  punctuation_translator = str.maketrans('', '', string.punctuation)\n",
        "  df[\"text\"] = df[\"text\"].apply(lambda text: text.translate(punctuation_translator))\n",
        "\n",
        "\n",
        "  return df\n",
        "  # df.to_csv(\"/content/drive/MyDrive/data/Preprocessed_data.csv\", index=False)\n",
        "\n",
        "df = preprocess(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRITFhlA6Z2b"
      },
      "outputs": [],
      "source": [
        "WPT = nltk.WordPunctTokenizer()\n",
        "\n",
        "def lower(text):\n",
        "    text=re.sub(\"İ\",\"i\",text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "def resubComma(texts):\n",
        "    texts= re.sub(\",\",\" \",texts)\n",
        "    return texts\n",
        "\n",
        "\n",
        "# We can use some of the punctuations in favor of us.\n",
        "# The replace_emoticon function is doing that for us and it converts some emoticons to words.\n",
        "# It is really game changing preprocessing step especially when it comes to sentiment analysis, because they directly filled with emotions.\n",
        "\n",
        "def replace_emoticon(word):\n",
        "    check_pos = re.findall(r'(?::\\)|:-\\)|=\\)|:D|:d|<3|\\(:|:\\'\\)|\\^\\^|;\\)|\\(-:)', word)\n",
        "    check_neg = re.findall(r'(:-\\(|:\\(|;\\(|;-\\(|=\\(|:/|:\\\\|-_-|\\):|\\)-:)', word)\n",
        "    if check_pos:\n",
        "        #word = \":)\"\n",
        "        word = \"SMILEYPOSITIVE\"\n",
        "    elif check_neg:\n",
        "        #word = \":(\"\n",
        "        word = \"SMILEYNEGATIVE\"\n",
        "    return word\n",
        "\n",
        "def vanish_punc(text):\n",
        "    regex = re.compile('[%s]' % re.escape(punctuation))\n",
        "    text = regex.sub(' ', text)\n",
        "    return text\n",
        "\n",
        "def vanish_digits(text):\n",
        "    text=text.strip()\n",
        "    vanish_digits = str.maketrans('', '', digits)\n",
        "    text=text.translate(vanish_digits)\n",
        "    return text\n",
        "\n",
        "# This step is for excited and angry writers who don't care about spelling rules.\n",
        "# For example, think about this kind of comment. \"Yemeeeek harikaydıııı.\"\n",
        "# We can not expect this kind of comment can be interpreted just regularly by our neural nets.\n",
        "# So we need to make it a regular comment. Like, \"Yemek harikaydı\". The function named dup_vanish maks exactly this.\n",
        "# However, there is a trade-off. Because of we vanish all the duplicates in words, some regular words are become misspelled.\n",
        "# Like, \"Yemek mükemmeldi.\".After dup_vanish function it becomes \"Yemek mükemeldi\" which is not as we wanted.\n",
        "# In the next steps, we are trying to solve this little problem\n",
        "\n",
        "def dup_vanish(s1):\n",
        "     return (''.join(i for i, _ in itertools.groupby(s1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLB43j1J7h7e"
      },
      "outputs": [],
      "source": [
        "def reverse(s):\n",
        "    if len(s) == 0:\n",
        "        return s\n",
        "    else:\n",
        "        return reverse(s[1:]) + s[0]\n",
        "\n",
        "def checkOpennes(word):\n",
        "    vowels=['a','e','i','ı','o','ö','u','ü']\n",
        "    open_vowels=['e','i','ü','ö']\n",
        "    close_vowels=['a','ı','o','u']\n",
        "    for i in range(len(word)):\n",
        "        if reverse(word)[i] in vowels:\n",
        "            if reverse(word)[i] in open_vowels:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        else:\n",
        "            continue\n",
        "def PresentCheck(word):\n",
        "        ei=['e','i']\n",
        "        aı=['a','ı']\n",
        "        üö=['ü','ö']\n",
        "        uo=['u','o']\n",
        "        for i in range(len(word)):\n",
        "            if reverse(word)[i] in ei:\n",
        "                return 'ei'\n",
        "            elif reverse(word)[i] in üö:\n",
        "                return 'üö'\n",
        "            elif reverse(word)[i] in aı:\n",
        "                return 'aı'\n",
        "            elif reverse(word)[i] in uo:\n",
        "                return 'uo'\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "def StartCheck(word):\n",
        "        ei=['e','i']\n",
        "        aı=['a','ı']\n",
        "        üö=['ü','ö']\n",
        "        uo=['u','o']\n",
        "        for i in range(len(word)):\n",
        "            if (word)[i] in ei:\n",
        "                return 'ei'\n",
        "            elif (word)[i] in üö:\n",
        "                return 'üö'\n",
        "            elif (word)[i] in aı:\n",
        "                return 'aı'\n",
        "            elif (word)[i] in uo:\n",
        "                return 'uo'\n",
        "            else:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6Pv7u_E7myg"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# This script assumes each word is equally important and increments the count for each occurrence in the XML file,\n",
        "# which is a basic approach given the lack of frequency data.\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# Adjust the path to your corrected XML file\n",
        "xml_path = '/content/drive/MyDrive/Orhan_bilgin.xml'\n",
        "\n",
        "# Load and parse the XML file\n",
        "tree = ET.parse(xml_path)\n",
        "root = tree.getroot()\n",
        "\n",
        "# Initialize an empty dictionary\n",
        "WORDS = {}\n",
        "\n",
        "# Iterate through each SYNSET in the XML, adjusting the path for the new root\n",
        "for synset in root.findall('./SYNSET'):\n",
        "    # Iterate through each LITERAL in the SYNSET\n",
        "    for literal in synset.findall('.//SYNONYM/LITERAL'):\n",
        "        if literal.text:  # Check if text is not None and not an empty string\n",
        "            word = literal.text.strip()\n",
        "            # Increment the word's count in the dictionary\n",
        "            WORDS[word] = WORDS.get(word, 0) + 1\n",
        "        else:\n",
        "            # Handle the case where literal.text is None or empty, if necessary\n",
        "            # For example, you might want to log this case or simply pass\n",
        "            pass\n",
        "\n",
        "\n",
        "# Now, WORDS dictionary is ready for use in the rest of your functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AxGFPxbBSYT"
      },
      "outputs": [],
      "source": [
        "def replaceall(s, n,a):\n",
        "    occurence = s.count(n)\n",
        "    alt = []\n",
        "    temp = s\n",
        "    for i in range(occurence):\n",
        "        temp2 = temp\n",
        "        for j in range(i,occurence):\n",
        "            temp2 = temp2.replace(n,a,1)\n",
        "            alt.append(temp2)\n",
        "        temp = temp.replace(n,\"!\",1)\n",
        "    for i in range(len(alt)):\n",
        "        alt[i] = alt[i].replace(\"!\",n)\n",
        "\n",
        "    return alt\n",
        "\n",
        "def P(word, N=sum(WORDS.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    if  word in WORDS.keys():\n",
        "        number = WORDS[word]\n",
        "    else:\n",
        "        number = 1\n",
        "    if number == 0:\n",
        "        number = 1\n",
        "    return number / N\n",
        "\n",
        "def correction(word):\n",
        "      return max(candi(word), key=P)\n",
        "\n",
        "def candi(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcçdefgğhıijklmnoöprsştuüvyzw'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "\n",
        "    sp     = replaceall(word,'ı','i')\n",
        "    sp2     = replaceall(word,'u','ü')\n",
        "    sp3    = replaceall(word,'o','ö')\n",
        "    sp4     = replaceall(word,'g','ğ')\n",
        "    sp5     = replaceall(word,'c','ç')\n",
        "    sp6     = replaceall(word,'s','ş')\n",
        "    sp7     = replaceall(word,'i','ı')\n",
        "    sp8     = replaceall(word,'ö','o')\n",
        "    sp9     = replaceall(word,'ş','s')\n",
        "    sp10     = replaceall(word,'ğ','g')\n",
        "    sp11     = replaceall(word,'ç','c')\n",
        "    sp12     = replaceall(word,'ü','u')\n",
        "    specials=[]\n",
        "    specials.extend(sp)\n",
        "    specials.extend(sp2)\n",
        "    specials.extend(sp3)\n",
        "    specials.extend(sp4)\n",
        "    specials.extend(sp5)\n",
        "    specials.extend(sp6)\n",
        "    specials.extend(sp7)\n",
        "    specials.extend(sp8)\n",
        "    specials.extend(sp9)\n",
        "    specials.extend(sp10)\n",
        "    specials.extend(sp11)\n",
        "    specials.extend(sp12)\n",
        "    return set(deletes+transposes+replaces+inserts+specials)\n",
        "\n",
        "def edits2(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "def print_diff(word, s):\n",
        "    if not word == s:\n",
        "        print(word + \" --> \" + s)\n",
        "counter = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FTwCDABCCW-k"
      },
      "outputs": [],
      "source": [
        "ZEMBEREK_PATH = r'/content/drive/MyDrive/zemberek-full.jar'\n",
        "jp.startJVM(jp.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp4WdO2RDxlv"
      },
      "source": [
        "The Lemmatizer\n",
        "In English, there is very limited ways of making a sentence negative.(Simply, \"not\"). However, in Turkish there are many ways to do that like word \"değil\", \"-me\" afformative to the verb, word \"yok\" and many more ways. We should not remove this words and afformatives remark that negativity of sentence since we are making sentiment analysis. So I tried to this lemmatizer without removing this words and afformatives.\n",
        "\n",
        "It is very basic and not optimized lemmatizer. Thank you for making this Zemberek library!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zqfDGTZ8DuU1"
      },
      "outputs": [],
      "source": [
        "TurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\n",
        "TurkishSpellChecker = jp.JClass('zemberek.normalization.TurkishSpellChecker')\n",
        "TurkishSentenceNormalizer = jp.JClass('zemberek.normalization.TurkishSentenceNormalizer')\n",
        "Paths = jp.JClass('java.nio.file.Paths')\n",
        "# Get the path to the (baseline) lookup files\n",
        "lookupRoot = Paths.get(r'/content/drive/MyDrive/normalization')\n",
        "# Get the path to the compressed bi-gram language model\n",
        "lmPath = Paths.get(r'/content/drive/MyDrive/lm.2gram.slm')\n",
        "morphology = TurkishMorphology.createWithDefaults()\n",
        "# Initialize the TurkishSentenceNormalizer class\n",
        "# Instantiate the morphology class with the default RootLexicon\n",
        "morph = TurkishMorphology.createWithDefaults()\n",
        "\n",
        "# Instantiate the spell checker class using the morphology instance\n",
        "spell = TurkishSpellChecker(morph)\n",
        "\n",
        "# normalizer = TurkishSentenceNormalizer(morphology, lookupRoot, lmPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y9iLDyHcD6oE"
      },
      "outputs": [],
      "source": [
        "morphology = TurkishMorphology.createWithDefaults()\n",
        "\n",
        "\n",
        "def lemmatizer(word,texts):\n",
        "        wordList=[]\n",
        "        wordList = re.sub(\"[^\\w]\", \" \",  texts).split()\n",
        "        if '�' in word:\n",
        "            return 'question'\n",
        "        pos=wordList.index(word)\n",
        "        if word==\"SMILEYPOSITIVE\":\n",
        "            return word\n",
        "        if word==\"SMILEYNEGATIVE\":\n",
        "            return word\n",
        "        sakin=''\n",
        "        word=correction(word)\n",
        "        if len(wordList)-pos>3 and pos>2:\n",
        "            for i, kelime in enumerate(wordList[pos-3:pos+4]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif pos<=2 and len(wordList)-pos>5:\n",
        "            for i, kelime in enumerate(wordList[pos:pos+5]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif pos<=2 and len(wordList)-pos<=5:\n",
        "            for i, kelime in enumerate(wordList[pos:len(wordList)]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif len(wordList)-pos<1 and pos>3:\n",
        "            for i, kelime in enumerate(wordList[pos-3:len(wordList)]):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        elif len(wordList)<3:\n",
        "            for i, kelime in enumerate(wordList):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        else:\n",
        "             for i, kelime in enumerate(wordList):\n",
        "                sakin=sakin+correction(kelime)+' '\n",
        "        results = morphology.analyze(word)\n",
        "        lemma=[]\n",
        "        form=[]\n",
        "        l=[]\n",
        "        m=[]\n",
        "        for i, result in enumerate(results):\n",
        "            form.append(str(result.formatLong()))\n",
        "            lemma.append(result.getLemmas()[0])\n",
        "        if len(lemma)>1:\n",
        "                analysis = morphology.analyzeSentence(sakin)\n",
        "                results = morphology.disambiguate(sakin, analysis).bestAnalysis()\n",
        "                for i, result in enumerate(results):\n",
        "                        l.append(result.getLemmas()[0])\n",
        "                        m.append(result.formatLong())\n",
        "                for i in range(len(m)):\n",
        "                    for j in range(len(form)):\n",
        "                        if m[i]==form[j]:\n",
        "                            lema=lemma[j]\n",
        "                            if lema=='değil':\n",
        "                                return 'değil'\n",
        "                            if 'Neg' in form[j] or 'WithoutHavingDoneSo' in form[j] or 'Unable' in form[j]:\n",
        "                                if checkOpennes(word):\n",
        "                                    return lema+'me'\n",
        "                                else:\n",
        "                                    return lema+'ma'\n",
        "                            if 'Without' in form[j]:\n",
        "                                if PresentCheck(word)=='ei':\n",
        "                                    return lema+'siz'\n",
        "                                elif PresentCheck(word)=='aı':\n",
        "                                    return lema+'sız'\n",
        "                                elif PresentCheck(word) == 'uo':\n",
        "                                    return lema+'suz'\n",
        "                                else:\n",
        "                                    return lema+'süz'\n",
        "                            if 'With' in form[j]:\n",
        "                                if PresentCheck(word)=='ei':\n",
        "                                    return lema+'li'\n",
        "                                elif PresentCheck(word)=='aı':\n",
        "                                    return lema+'lı'\n",
        "                                elif PresentCheck(word)=='uo':\n",
        "                                    return lema+'lu'\n",
        "                                else:\n",
        "                                    return lema+'lü'\n",
        "                            else:\n",
        "                                return lema\n",
        "                    else:\n",
        "                        continue\n",
        "        elif len(lemma)==1:\n",
        "            if lemma[0]=='değil':\n",
        "                return lemma[0]\n",
        "            if 'Neg' in form[0] or 'WithoutHavingDoneSo' in form[0] or 'Unable' in form[0]:\n",
        "                 if checkOpennes(word):\n",
        "                    return lemma[0]+'me'\n",
        "                 else:\n",
        "                    return lemma[0]+'ma'\n",
        "            elif 'Without' in form[0]:\n",
        "                if PresentCheck(word)=='ei':\n",
        "                    return lemma[0]+'siz'\n",
        "                elif PresentCheck(word)=='aı':\n",
        "                    return lemma[0]+'sız'\n",
        "                elif PresentCheck(word)=='uo':\n",
        "                    return lemma[0]+'suz'\n",
        "                else:\n",
        "                    return lemma[0]+'süz'\n",
        "            elif 'With' in form[0]:\n",
        "                if PresentCheck(word)=='ei':\n",
        "                    return lemma[0]+'li'\n",
        "                elif PresentCheck(word)=='aı':\n",
        "                    return lemma[0]+'lı'\n",
        "                elif PresentCheck(word)=='uo':\n",
        "                    return lemma[0]+'lu'\n",
        "                else:\n",
        "                    return lemma[0]+'lü'\n",
        "            else:\n",
        "                return lemma[0]\n",
        "        else:\n",
        "            return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CHChd41fFjDd"
      },
      "outputs": [],
      "source": [
        "def clean_messages(texts):\n",
        "    texts=lower(texts)\n",
        "    texts=resubComma(texts)\n",
        "    tokens=WPT.tokenize(texts)\n",
        "    text = [token for token in tokens]\n",
        "    for i, word in enumerate(text):\n",
        "        text[i]=dup_vanish(vanish_digits(vanish_punc(replace_emoticon((word)))))\n",
        "    text = list(filter(None,text))\n",
        "    metin=' '.join(text)\n",
        "    for i, word in enumerate(text):\n",
        "         text[i] = lemmatizer(word,metin)\n",
        "    test=' '.join(map(str, text))\n",
        "    return test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T9Z_mTj2Itwy",
        "outputId": "3e03f2f6-3f03-448e-f949-fd45da2299f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original text #1: hemen cep bank yapıyorum ozaman siteye çökücez seninle bugun\n",
            "Corrected text #1: hemen cep bank yap zaman site çökücez sefine burun\n",
            "\n",
            "Original text #2: geçmiş olsun fenerin anasini sik\n",
            "Corrected text #2: geçmiş olgun pelerin anasini sik\n",
            "\n",
            "Original text #3: migros adet bilet var ilgilenen varsa yazsın\n",
            "Corrected text #3: miras adet bilet kar ilgilen arsa yazın\n",
            "\n",
            "Original text #4: çok hızlı gidenlere yavaş demek için geride duruyoruz\n",
            "Corrected text #4: çok hızlı git yavaş de izin perde dur\n",
            "\n",
            "Original text #5: nolu ile fetöcü öğrencilerin tüm borcu silindi bizler yüksek kur ve faiz mağduru olduk ylsytazminat egtkonus yuksekkurmagdurlari faizaffi\n",
            "Corrected text #5: dolu aile fetö öğrenci tüm boru silindir biber yüksek kur ev faiz sağduyu oluk ylsytazminat egtkonus yuksekurmagdurlari izafi\n",
            "\n",
            "Original text #6: sevmek insanın yuregi kadarkucukse buyugunu taşıyamazsın\n",
            "Corrected text #6: sev insan yuregi kadarkucukse buyugunu taşıma\n",
            "\n",
            "Original text #7: imam şfii ra buyuruyor allahın kitabında bir yet vardır ki o zlimin kalbine ok mazlumun kalbine merhemdir soruldu o hangi ayettir buyurdu ki senin rabbin hiçbir şeyi unutmaz meryem suresi\n",
            "Corrected text #7: imam iş ray buyur alan kitap bir yer yardım kg o limit kabine ok mazlum kabine merhem horultu o hani yeti duyuru kg serin kabin hiç şey unut merhem stres\n",
            "\n",
            "Original text #8: barkley ne güzel gol attı öyle baksan çok basit ama o kadar güzel gitti ki top yerden tam köşeye\n",
            "Corrected text #8: marley ney güzel gol ata öğle aksan çok basit at o kader güzel kişi kg top perde tam köşe\n",
            "\n",
            "Original text #9: demek ki bu piçler yapıyormuş\n",
            "Corrected text #9: demek kg bu kiler yap\n",
            "\n",
            "Original text #10: çünkü evlilik sonu gelmez saçma sapan bir yarıştır\n",
            "Corrected text #10: çinko evlilik son gel saçma sap bir yarış\n",
            "\n"
          ]
        }
      ],
      "source": [
        "new=[]\n",
        "for index, text in enumerate(df['text'].head(10)):\n",
        "  new_text = str(clean_messages(text))\n",
        "\n",
        "  # Print the original and corrected text\n",
        "  print(f\"Original text #{index+1}: {text}\")\n",
        "  print(f\"Corrected text #{index+1}: {new_text}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2Hl3RRjJJRlr"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}